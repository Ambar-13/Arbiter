cff-version: 1.2.0
message: "If you use ConstrAI in your research, please cite it as below."
title: "ConstrAI: Formal Safety Framework for AI Agents with Provable Guarantees"
type: software
authors:
  - given-names: Ambar
    affiliation: "National University of Singapore"
    orcid: ""
version: "0.4.1"
date-released: "2026-02-27"
license: MIT
url: "https://github.com/Ambar-13/ConstrAI"
repository-code: "https://github.com/Ambar-13/ConstrAI"
description: >-
  A formal safety framework for AI agents that enforces mathematical
  invariants through state-transition simulation rather than prompt-based
  guardrails. Provides eight proven safety theorems (T1-T8: budget safety,
  termination, invariant preservation, monotone resources, atomicity, trace
  integrity, rollback exactness, and emergency override) enforced by a
  non-bypassable safety kernel. Ships with 25 pre-built invariant factories,
  LLM adapters for Anthropic, OpenAI, OpenClaw, LangChain, and MCP, and a
  constrai.testing module for writing safety property tests. Evaluated
  against 39 adversarial attack scenarios across 9 threat categories with
  89.7% recall and zero false positives at sub-millisecond latency
  (45,000+ checks/sec). Zero runtime dependencies.
keywords:
  - ai-safety
  - agent-safety-framework
  - formal-verification
  - autonomous-agents
  - invariant-checking
  - llm-safety
  - mathematical-constraints
  - state-machines
  - execution-semantics
